{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GalasAutoTagChallenge",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO3SvvDSonDRKaisHYkJ9H9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/six-ten/image-classification-notebooks/blob/master/GalasAutoTagChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3pI3kf4BQ-c",
        "colab_type": "text"
      },
      "source": [
        "# dataset is expected to be present in a folder called Datasets as dataset.zip in gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqgHWgIUOP2H",
        "colab_type": "code",
        "outputId": "a5f51813-4769-48f4-942d-4812f3715a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/gdrive')\n",
        "dataset_path = '/content/gdrive/My Drive/Datasets/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jj3DRpySMXZ",
        "colab_type": "code",
        "outputId": "c6edf8e2-8663-4ea7-f46b-0a68ebe670df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models,layers,applications, optimizers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBN9hVaCBD89",
        "colab_type": "text"
      },
      "source": [
        " # creating a local copy of dataset for faster IO , loading 1000s of images over network directly from gdrive can be a bottleneck\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDtNnmjnK_Ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cP9OY5HMtdj",
        "colab_type": "code",
        "outputId": "25e70acb-7451-4553-bf6e-0dce4349faba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir LocalStorage"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘LocalStorage’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOc7vmMyNYqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r 'content/gdrive/My Drive/Datasets/dataset.zip' '/LocalStorage/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCctWi3VXL7-",
        "colab_type": "code",
        "outputId": "534b3521-b823-46d1-bc03-379e226d844e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.listdir('./LocalStorage/')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dataset', 'dataset.zip']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVAxBM8qNoBg",
        "colab_type": "code",
        "outputId": "2971ddb8-b6b6-4f01-b34f-6d20ad2bfe8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "source": [
        "!unzip -q \"/LocalStorage/dataset.zip\" -d \"/LocalStorage/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace /LocalStorage/dataset/test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kGwaJtNOJmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/LocalStorage/dataset/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p3y1Np3XRgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('./train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VBjw1I3zjaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv('./test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeSGIfA2Q35B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyrWdFzk0KVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROXy5PF9YWwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_counts = dict(train_df['Class'].value_counts())\n",
        "print(class_counts)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLwT0I8c34uQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_labels = {'Food': 0, 'Attire': 1, 'misc': 2, 'Decorationandsignage': 3}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heZ-pseSBfHY",
        "colab_type": "text"
      },
      "source": [
        "# it can be seen that the dataset has unequal number of samples from each class, so we calculate weight factors to calculate weighted loss to compensate for imbalance in distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbIU_On73zvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights = {class_labels[key]: sum(class_counts.values())/class_counts[key] for key in class_counts.keys()}\n",
        "print(class_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_Ocy2Cz9zi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this dictionary will be used to map predictions with labels\n",
        "predict_dict  = {value:key for key, value in class_labels.items()}\n",
        "print(predict_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iakD_RhEB4Fm",
        "colab_type": "text"
      },
      "source": [
        "# an unused function that is supposed to randomly sample and upsample dataset to account for imbalance in distribution, returns a dataframe with duplicated images, marked True for augment flag, images with that flag are duplicates and should be modified in some way (augmentation) if this method is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LgZsColQLcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def upsample_before_train(df):\n",
        "  class_counts = dict(train_labels['Class'].value_counts())\n",
        "  df_dict = filter_DataFrame(df)\n",
        "  ref_count = max(list(class_counts.values()))\n",
        "  for cls, df in df_dict.items():\n",
        "        class_df = df_dict[cls]\n",
        "        instance_count = class_df.shape[0]\n",
        "        scale_factor = ref_count/instance_count\n",
        "        for _ in range(int(scale_factor)-1):\n",
        "          df_dict[cls] = df_dict[cls].append(class_df.sample(class_counts[cls]))\n",
        "        df_dict[cls] = df_dict[cls].append(class_df.sample(int(class_counts[cls]*(scale_factor-int(scale_factor)))))\n",
        "        df_dict[cls]['augment'] = df_dict[cls].duplicated()\n",
        "        df_dict[cls] = shuffle(df_dict[cls])\n",
        "  return df_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwGg7TENzTVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smart_crop(img,thresh=0.8,mode='split'):\n",
        "  \n",
        "  h,w = img.shape[:-1]\n",
        "\n",
        "  if min([w,h])/max(w,h) > thresh :\n",
        "    return [img]\n",
        "  else :\n",
        "    if h>w :\n",
        "      if mode == 'center':\n",
        "        h2 = h//2\n",
        "        w2 = w//2\n",
        "        return [img[h2-w2:h2+w2,:]]\n",
        "      return [img[:w,:],img[-w:]]\n",
        "    else :\n",
        "      if mode == 'center':\n",
        "        w2 = w//2\n",
        "        h2 = h//2\n",
        "        return [img[:,w2-h2:w2+h2]]\n",
        "      return [img[:,:h],img[:,-h:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3KHkf7rbSgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_datav1(dfs,image_dir_names,output_shape=(224,224),images_only=False):\n",
        "  '''functions specific to this example\n",
        "  df : dataframe containing input data\n",
        "  image_dir_name : name of folder containing images\n",
        "  output_shape : output size of images after preprocessing\n",
        "  images_only : flag to determine the output true -> returns images only -> for running predictions\n",
        "                false -> return labels and images -> for training\n",
        "  \n",
        "   '''\n",
        "  imgs = []\n",
        "  lbls = []\n",
        "  f = 2 ;\n",
        "  if images_only :\n",
        "    f = 1 \n",
        "  for df,image_dir_name in zip(dfs,image_dir_names):\n",
        "    \n",
        "    df_dict = df.to_dict()\n",
        "    images = df_dict['Image']\n",
        "    if not images_only:\n",
        "      labels = df_dict['Class']\n",
        "    for key,img in images.items():\n",
        "      \n",
        "      Im = cv2.imread('./'+image_dir_name+'/'+img)\n",
        "      Image_heights.append(Im.shape[0])\n",
        "      Image_widths.append(Im.shape[1])\n",
        "      Ims = smart_crop(Im)\n",
        "      #Ims = [Im]\n",
        "      Ims = [cv2.resize(x,output_shape) for x in Ims]\n",
        "      for Im in Ims:\n",
        "        imgs.append(Im)\n",
        "        if not images_only:\n",
        "          imgs.append(cv2.flip(Im,1))\n",
        "      if not images_only :\n",
        "        for _ in range(f*len(Ims)):\n",
        "          lbls.append(class_labels[labels[key]])\n",
        "  imgs = applications.vgg16.preprocess_input(np.array(imgs))\n",
        "  if images_only :\n",
        "    return imgs\n",
        "  else :\n",
        "    return imgs,np.array(lbls)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9agjMDbkeoyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image_heights = []\n",
        "Image_widths = []\n",
        "def get_data(dfs,image_dir_names,output_shape=(224,224),images_only=False):\n",
        "  global Image_heights, Image_widths\n",
        "  '''functions specific to this example\n",
        "  df : dataframe containing input data\n",
        "  image_dir_name : name of folder containing images\n",
        "  output_shape : output size of images after preprocessing\n",
        "  images_only : flag to determine the output true -> returns images only -> for running predictions\n",
        "                false -> return labels and images -> for training\n",
        "  \n",
        "   '''\n",
        "  imgs = []\n",
        "  lbls = []\n",
        "  f = 2 ;\n",
        "  if images_only :\n",
        "    f = 1 \n",
        "  for df,image_dir_name in zip(dfs,image_dir_names):\n",
        "    \n",
        "    df_dict = df.to_dict()\n",
        "    images = df_dict['Image']\n",
        "    if not images_only:\n",
        "      labels = df_dict['Class']\n",
        "    for key,img in images.items():\n",
        "      \n",
        "      Im = cv2.imread('./'+image_dir_name+'/'+img)\n",
        "      Image_heights.append(Im.shape[0])\n",
        "      Image_widths.append(Im.shape[1])\n",
        "      Ims = smart_crop(Im,mode='center')\n",
        "      #Ims = [Im]\n",
        "      Im = cv2.resize(Im,output_shape)\n",
        "      imgs.append(Im)\n",
        "      if not images_only :\n",
        "        lbls.append(class_labels[labels[key]])\n",
        "  imgs = applications.vgg16.preprocess_input(np.array(imgs))\n",
        "  if images_only :\n",
        "    return imgs\n",
        "  else :\n",
        "    return imgs,np.array(lbls)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYMNy0z1eqSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train, Y_train = get_data([train_df,predictions],['Train Images','Test Images'])\n",
        "X_train, Y_train = get_datav1([train_df.sample(2000)],['Train Images'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_N61lsD-VpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG5ndLTE5BV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_t_df = pd.DataFrame(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QGDp_2Km3-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_counts = Y_t_df[0].value_counts().to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdyUINEF6SCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHPFc-S75otZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aug_class_weights = {key: sum(Y_train_counts.values())/Y_train_counts[key] for key in Y_train_counts.keys()}\n",
        "print(aug_class_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU0lBBtKzC-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRonb4vUfKBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_predict = get_data([test_df],['Test Images'],images_only=True)\n",
        "print(X_predict.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_HeWUMIf7KB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Y_train_ohv = tf.keras.utils.to_categorical(Y_train,len(class_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HQOya-VtiJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_custom_model(input_shape=(80,80,3),n_classes=4):\n",
        "  new_model = models.Sequential()\n",
        "  new_model.add(layers.Input(shape=input_shape))\n",
        "  new_model.add(layers.Conv2D(64,5))\n",
        "  new_model.add(layers.Conv2D(128,5))\n",
        "  new_model.add(layers.Conv2D(128,5))\n",
        "  new_model.add(layers.MaxPool2D((2,2)))\n",
        "  new_model.add(layers.BatchNormalization())\n",
        "  new_model.add(layers.Conv2D(128,3))\n",
        "  new_model.add(layers.Conv2D(128,3))\n",
        "  new_model.add(layers.Flatten())\n",
        "  new_model.add(layers.Dense(1024,activation='relu'))\n",
        "  new_model.add(layers.Dense(n_classes,activation='softmax'))\n",
        "  return new_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbMYqhEi0gxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createTLmodel(x_train,y_train,n_classes = 4,fraction=0.9,params):\n",
        "  \n",
        "  net = applications.VGG16()\n",
        "  new_model = models.Sequential()\n",
        "  split_layer_index = int(fraction*len(net.layers))\n",
        "  for i,layer in enumerate(net.layers[:-1]) :\n",
        "    new_model.add(layer)\n",
        "    if (i<split_layer_index):\n",
        "      layer.trainable = False\n",
        "      #pass\n",
        "  '''new_model.add(layers.Dense(512,activation='relu'))\n",
        "  new_model.add(layers.Dropout(0.5))\n",
        "  new_model.add(layers.Dense(512,activation='relu'))\n",
        "  new_model.add(layers.Dropout(0.5))'''\n",
        "  new_model.add(layers.Dense(n_classes,params['activation']))\n",
        "  new_model.compile(loss='categorical_crossentropy',optimizer=params['optimizer'])\n",
        "  out = new_model.fit(x_train,y_train,batch_size=params['batch_size'],epochs=params['epochs']\n",
        "                      validation_split = params['validation_split']\n",
        "                      )\n",
        "  return out,new_model\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g7wFNCDbajE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unfreeze_from_last(model):\n",
        "  for layer in model.layers[::-1]:\n",
        "    if not layer.trainable :\n",
        "      layer.trainable = True\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krascD6pkvRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def freeze_from_last(model):\n",
        "  for layer in model.layers:\n",
        "    if layer.trainable == True :\n",
        "      layer.trainable = False\n",
        "      break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suPrTLB3SJ7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(act,opt,lr,n_classes=4):\n",
        "  opts = {\n",
        "      'adam':optimizers.Adam(lr),\n",
        "      'sgd':optimizers.SGD(lr)\n",
        "  }\n",
        "  opt = opts[opt]\n",
        "  net = applications.VGG16()\n",
        "  new_net = models.Sequential()\n",
        "  for layer in net.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "    new_net.add(layer)\n",
        "    \n",
        "  new_net.add(layers.Dense(4,activation=act))\n",
        "  unfreeze_from_last(new_net)\n",
        "  unfreeze_from_last(new_net)\n",
        "  new_net.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "  return new_net\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spMPcoyiGDU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluations=[]\n",
        "done = set()\n",
        "def Scan(data_x,data_y,model_function,params,validation_split=0.2):\n",
        "  global evaluations,done\n",
        "  \n",
        "  evaluations = []\n",
        "  l = int(validation_split*len(data_x))\n",
        "  train_x = data_x[:l]\n",
        "  train_y = data_x[:l]\n",
        "  val_x = data_x[l:]\n",
        "  val_y = data_y[l:]\n",
        "  activations  =params['activation']\n",
        "  optimizers = params['optimizers']\n",
        "  learning_rates = params['learning_rates']\n",
        "  for act in activations :\n",
        "    for opt in optimizers :\n",
        "      for lr in learning_rates :\n",
        "        \n",
        "        eval_key = opt+act+str(lr)\n",
        "        if eval_key in done :\n",
        "          continue\n",
        "        print('training config : ',opt,' ',act,' ',lr)\n",
        "        model = model_function(act,opt,lr)\n",
        "        result = model.fit(data_x,data_y,batch_size=16,epochs=3,class_weight=aug_class_weights,validation_split=validation_split)\n",
        "        tf.reset_default_graph()\n",
        "        tf.keras.backend.clear_session()\n",
        "        evaluations.append([act,opt,lr,result])\n",
        "        done.add(eval_key)\n",
        "        print(result)\n",
        "  return evaluations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT8tSse1YwPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {\n",
        "    'optimizers':['adam','sgd'],\n",
        "    'learning_rates':[0.001,0.0001,0.01,0.1],\n",
        "    'activation':['softmax']\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTOw08smZT9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evals = Scan(X_train[:2000],Y_train[:2000],create_model,parameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkFEPP9wj4Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_mr76YioaFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_prediction_csv(my_model=my_model):\n",
        "  res = my_model.predict(X_predict)\n",
        "  res = np.argmax(res,axis=1)\n",
        "  new_df = test_df.copy(deep=True)\n",
        "  new_df['Class'] = [predict_dict[x] for x in res]\n",
        "  new_df.to_csv('/content/gdrive/My Drive/Datasets/prediction3.csv',index=False)\n",
        "  return new_df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QayDJ0RekJQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S5rQHfvZiPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.get_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJbQfiJig7PL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e1 = evaluations[0][-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEzHNVulg_Y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZcHa3MjhAXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOX0cl7ImDQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ps -aux|grep python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs8PkjN3mJp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 24 772 1267 1269"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS4jJRExhBkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e1.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu_mRhoWhJs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}